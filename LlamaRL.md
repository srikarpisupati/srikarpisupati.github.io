
# LlamaRL Paper Summary

## What is the problem the paper is trying to solve?
This paper introduces a fully distributed, asynchronous RL training framework for efficient and large-scale LLM training on large GPU clusters. This is used to manage policy models with hundreds of billions of parameters, which are used to improve LLM capabilities. This framework empirically and theoretically leads to speed-ups. 

This framework accommodates various RL algorithms, each bringing its own models with it (actor, critic, reward, etc). This framework also allows for parallelism to train large models. The framework handles various models of various sizes running simultaneously on GPU clusters containing a variety of devices. Finally, it tries to minimize idle bubbles, which happen because different responses on different devices can have different lengths and thus take different amounts of time. 

The reason for this framework is because of findings that scaling inference-time compute (for RL) is an effective approach for empowering LLMs with reasoning and alignment. 

## The main idea

LlamaRL uses a single-controller architecture built on PyTorch. The core set of best practices introduced are:
Co-located model offloading with Fine-grained Parallelism and Quantization, Asynchronous off-policy RL training, and GPU-native fully distributed weight synchronization via direct memory access.

The online RL training flow example used in this paper is one that involves a training policy --> an inference policy --> rule-based scorers --> and an RL algorithm (--> training policy).

## Purpose of design components and techniques

The paper adopts a distributed model placement strategy instead of co-located model placement. Although a co-located model placement strategy has its own advantages, such as lower communication latency, the main drawback is that each model would share a limited memory to generate and process data (forces models to share same parallelisms). Instead, the distributed strategy allocates each model to an individual processing group with exclusive devices. For example, the generation phase is offloaded to a dedicated group because it is a memory-bound process. Instead of co-locating the models with the same architecture but different weights (for example policy model and reference model), this paper develops a distributed direct memory access method to communicate weights (The bottleneck is not on the GPU inter-node bandwidth, but on the expensive weights reloading operation). The main reason for using this distributed model placement strategy is for asynchronous training.

However, there still may be idle bubbles and poor GPU utilization with distributed model placement, because models in RL training are still executed sequentially. This is solved through asynchronous off-policy RL. Each processing group parallelly runs and communicates after each training step. The trainer and generator compute concurrently, reducing training time. For further scaling up and mitigating straggler effects (within and across processing group),  long responses are segmented into smaller portions, and unfinished prompts are cached and resumed in future iterations. "Asynchronous design decomposes the shared memory constraint of the synchronous baseline into two independent constraintsâ€”one for the trainer and one for the generator", allowing each to be optimized separately. 

However, off-policyness is still possible (model is trained on samples generated by a previous iteration of itself), which are mitigated by off-policy corrections. 

Along with this, data parallel (training), model parallel (inference), pipeline parallel, and quantization (inference) are used to further optimize within processing groups.

The framework is built on top of PyTorch. It orchestrates several executors, each responsible for stages in the pipeline (inference, training, etc). There are communication channels to pass generations or weights between executors. Each executor has, under the hood, a distributed processing group.

Instead of using a Parameter Server to synchronize model weights between executors, this paper introduces Distributed Direct Memory Access, to work around insufficient network bandwidth or non-scaled software. Each GPU only stores or updates its assigned shards, eliminating memory bottlenecks on a single node. This technique also uses NVLink and bypasses the need for CPU memory. 


## Technical Limitations
A single controller being responsible for orchestrating the executors. This could cause issues at large scale, or when there are heterogenous GPUs. 


## How to improve limitations

A multi-controller solution could increase fault-tolerance, and configuring this controller to deal with heterogenous machines could improve performance. Perhaps this controller can be improved with added complexity to determine the best type of parallelism for a given GPU.  







